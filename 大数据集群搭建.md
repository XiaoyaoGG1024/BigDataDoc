------

# 大数据集群搭建

#### 集群启动

nn

http://hadoop01:9870/

2nn

http://hadoop02:9868/

rm

http://hadoop01:8088/

job

http://hadoop01:19888/

flink

http://hadoop01:8081/

hbase

http://hadoop01:16010/

帆软
http://hadoop01:37799/webroot/decision\

```shell
start-all.sh
mapred --daemon start historyserver
idea连接hive
hive --service metastore &
使用dbserver链接hive
hive --service hiveserver2 &
zk.sh start 
kf.sh start

/export/servers/FineBI6.0/bin

start-hbase.sh
```



#### 网络规划

|   节点   |       ip        | 节点类型 |
| :------: | :-------------: | :------: |
| hadoop01 | 192.168.202.134 |  master  |
| hadoop02 | 192.168.202.135 |  slave1  |
| hadoop03 | 192.168.202.136 |  slave2  |

#### 节点规划

|      服务       | Hadoop01 | Hadoop02 | Hadoop03 |
| :-------------: | :------: | :------: | :------: |
|    namenode     |    √     |          |          |
| secondNamenode  |          |    √     |          |
| resourcemanager |    √     |          |          |
|    datanode     |    √     |    √     |    √     |
|   nodemanager   |    √     |    √     |    √     |
|   jobhistory    |    √     |          |          |

#### 组件规划

|          |                             组件                             |
| :------: | :----------------------------------------------------------: |
| Hadoop01 | mysql,hive,spark,zk,ck,maxwell,hbase,flume,kafka,flink,redis，帆软 |
| Hadoop02 |                     zk,kafka,flink,hbase                     |
| Hadoop03 |                     zk,kafka,flink,hbase                     |

![image-20230522115620191](D:\集群规划\大数据集群搭建.assets\image-20230522115620191.png)

#### 搭建流程

###### 防火墙

```shell
查看防火墙状态
systemctl status firewalld
关闭防火墙
systemctl stop firewalld
```

###### 分发

```shell
cp 
scp -r 
```

###### 网络搭建

```shell
CentOS7和8里面，网络配置文件为：/etc/sysconfig/network-scripts/ifcfg-ens33  然后重启网卡

TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"

BOOTPROTO="static"

DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="fe58a64d-bd63-4195-873a-c3125b91f7bf"
DEVICE="ens33"
ONBOOT="yes"

IPADDR=192.168.202.134
NETMASK=255.255.255.0
GATEWAY=192.168.202.2
DNS1=8.8.8.8
```

```shell
vi  /etc/hosts

192.168.202.134 hadoop01
192.168.202.135 hadoop02
192.168.202.136 hadoop03
```

###### 修改主机名

```shell
hostnamectl set-hostname <newhostname>
bash 
```

###### 免密登录

```shell
ssh-keygen -t rsa  三台
三次回车
3台节点向其他两台外加本身  总共免密9次
ssh-copy-id hadoop01
ssh hadoop02 切换
```



###### 组件配置环境变量

######  vi /etc/profiled

```shell
export B_HOME=/export/servers
# Java
export JAVA_HOME=$B_HOME/jdk
export PATH=$PATH:$JAVA_HOME/bin
# Zookeeper
export ZOOKEEPER_HOME=$B_HOME/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
# Hadoop
export HADOOP_HOME=$B_HOME/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
# HIVE
export HIVE_HOME=$B_HOME/hive
export PATH=$PATH:$HIVE_HOME/bin
# Spark
export SPARK_HOME=$B_HOME/spark
export PATH=$PATH:$SPARK_HOME/bin
# Kafka
export KAFKA_HOME=$B_HOME/kafka
export PATH=$PATH:$KAFKA_HOME/bin
# Flume
export FLUME_HOME=$B_HOME/flume
export PATH=$PATH:$FLUME_HOME/bin
# Sqoop
export SQOOP_HOME=$B_HOME/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
# HBase
export HBASE_HOME=$B_HOME/hbase
export PATH=$PATH:$HBASE_HOME/bin
# Scala
export SCALA_HOME=$B_HOME/scala
export PATH=$PATH:$SCALA_HOME/bin
# Flink
export FLINK_HOME=$B_HOME/flink
export PATH=$PATH:$FLINK_HOME/bin


export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

```shell
 **source /etc/profiled**
```

###### JDK配置

```shell
tar -zxvf 文件名 -C 目标文件
java -version
```

###### HADOOP完全分布式配置

```shell
hadoop version 

修改配置文件
hadoop-env.sh
export JAVA_HOME=/export/servers/jdk

hdfs-site.xml
<configuration>
	<!--web UI-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>

    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop02:9868</value>
    </property>
    <!--副本数-->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
</configuration>

core-site.xml
<configuration>

<property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9000</value>
</property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/servers/hadoop/tmp</value>
</property>
<property>
		<name>hadoop.proxyuser.root.hosts</name>  
		<value>*</value> 
	</property>
	<property>
		<name>hadoop.proxyuser.root.groups</name>
		<value>*</value>
	</property>
    
</configuration>


mapred-site.xml
<configuration>

    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!--历史服务器-->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop01:19888</value>
</property>
</configuration>

workers  datanode
hadoop01
hadoop02
hadoop03

yarn-site.xml
<configuration>

<!-- Site specific YARN configuration properties -->
   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>

    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <!-- 一个容器获得的最小内存 -->
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <!-- 一个容器获得的最大内存 -->
        <value>6144</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <!-- 该节点上YARN可使用的物理内存总量 -->
        <value>6144</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个 -->
        <value>8</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <!-- 容器最小CPU核数，默认1个 -->
        <value>1</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <!-- 容器最大CPU核数，默认4个 -->
        <value>8</value>
    </property>
	<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
	<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

</configuration>
```

```shell
格式化 hdfs namenode - format 或 hadoop namenode - format
start-all.sh
mapred --daemon start historyserver
```

###### MYSQL安装

```shell
检查是否安装mysql
rpm -qa | grep mysql
rpm -e name 删除
下载mysql57-community-release-el7-7.noarch.rpm

安装MySQL包
yum -y install mysql57-community-release-el7-7.noarch.rpm
安装mysql需要禁掉GPG验证检查
yum update
yum -y install mysql-community-server --nogpgcheck
修改权限
chown -R mysql:mysql /var/lib/mysql
mysql初始化
mysqld --initialize
mysql启动
systemctl start mysqld
验证mysql安装
mysqladmin --version
登陆验证
第一次一般不需要密码 mysql直接进去 如果出现需要密码登陆报错请参考
https://blog.csdn.net/qq_54737884/article/details/122410476?spm=1001.2014.3001.5502
```

```shell
mysql报错
mysql5.0设置密码报错：ERROR 1193 (HY000): Unknown system variable
vi /etc/my.cnf
在mysqld加入
skip-grant-tables  #root免密登录
plugin-load-add=validate_password.so  #插件的加载方法，每次服务器启动时都必须给出该选项；
validate-password=FORCE_PLUS_PERMANENT #validate-password在服务器启动时使用该选项来控制插件的激活。
重启mysql
systemctl restart mysqld

设置密码等级
set global validate_password_policy=0;
查看安全策略 
SHOW VARIABLES LIKE 'validate_password%';
设置密码长度
set global validate_password_length=4;
设置新密码
SET PASSWORD = PASSwORD ('123456');
查看密码长度
select @@validate_password_length;
设置远程访问授权
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
刷新权限
flush privileges
```

###### HIVE安装

```shell
hive-site.xml

<configuration>
<!-- jdbc 连接的 URL -->
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
</property>
<!-- jdbc 连接的 Driver-->
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<!-- jdbc 连接的 username-->
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
</property>
<!-- jdbc 连接的 password -->
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
</property>
<!-- Hive 默认在 HDFS 的工作目录 -->
<property>
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
</property>
<!-- 指定存储元数据要连接的地址 -->
<property>
<name>hive.metastore.uris</name>
<value>thrift://hadoop01:9083</value>
</property>
<!--显示数据库名列明-->
<property>
<name>hive.cli.print.current.db</name>
<value>true</value>
</property>
<property>
<name>hive.cli.print.header</name>
<value>true</value>
</property>
</configuration>

将mysql驱动包拷贝到hive/lib目录下

初始化hive元数据库
schematool -dbType mysql -initSchema
初始化错误Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)
解决请参考厦门大学林子雨实验室
https://dblab.xmu.edu.cn/blog/?s=hive
初始化成功会出现
Initialization script completed
schemaTool completed

出现
FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
启动hive元数据服务
hive --service metastore
到此hive配置完成
```

###### SPARK ON YARN安装

```
将hive/conf/hive-site.xml 拷贝到 spark/conf/下
将mysql驱动包拷贝jars下

修改spark-env.sh
export JAVA_HOME=/export/servers/jdk
export SPARK_CONF_DIR=/export/servers/spark/conf
export HADOOP_CONF_DIR=/export/servers/hadoop
export YARN_CONF_DIR=/export/servers/hadoop/etc/hadoop

bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn  \
/export/servers/spark/examples/jars/spark-examples_2.12-3.1.1.jar 100
```

如果是本地idea测试 需要在本地设置hadoop的环境变量 其中要跟换二进制文件 winutils 

还需要配置本地hosts

需要将hive-site,core-site,hdfs-site,yarn-site拷贝到resouce下并配置log4j

```
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 
HH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell,
the
# log level for this class is used to overwrite the root logger's log level, so
that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent
UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```

```scala
import org.apache.spark.sql.SparkSession

/**
 * @program: BigData
 * @description: ${description}
 * @author: 逍遥哥哥每天都要努力啊
 * @create: 2023/4/28
 * */
object test {
  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME", "root")

    val spark = SparkSession
      .builder()
      .master("local[*]")
      .appName("regiontopthree")
      .config("spark.sql.warehouse.dir", "hdfs://hadoop01:9000/user/hive/warehouse")
      .config("hive.metastore.uris", "thrift://hadoop01:9083")
      .enableHiveSupport()
      .getOrCreate()


    spark.sql("show databases").show(false)

    spark.stop()
  }

}

```



###### ZOOKEEPER安装

```
vi z00.cfg
# 心跳检查的时间 2秒
tickTime=2000
# 初始化时 连接到服务器端的间隔次数，总时间10*2=20秒
initLimit=10
# ZK Leader 和follower 之间通讯的次数，总时间5*2=10秒
syncLimit=5
# 存储内存中数据快照的位置，如果不设置参数，更新事务日志将被存储到默认位置。
dataDir=/export/servers/zookeeper/ZKdata
# ZK 服务器端的监听端口  
clientPort=2181
# (心跳端口:选举端口)
server.1=hadoop01:2888:3888
server.2=hadoop02:2888:3888
server.3=hadoop03:2888:3888

vi myid  
1
分发三台
2 3
```

```shell
#!/bin/bash

case $1 in
"start"){
	for i in hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 启动 ------------
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in  hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 停止 ------------    
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in  hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 状态 ------------    
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh status"
	done
};;
esac

```

```
 chmod 777 zk.sh
```



###### FLINK本地模式

```shell
tar -zxvf
bin/start-cluster.sh
bin/stop-cluster.sh

[root@hadoop01 flink]# jps
3073 Jps
3014 TaskManagerRunner
2701 StandaloneSessionClusterEntrypoint
```

###### FLINK集群模式

进入 conf 目录下，修改 flink-conf.yaml 文件，修改 jobmanager.rpc.address 参数为 hadoop01  

这就指定了 hadoop01 节点服务器为 JobManager 节点。

```
jobmanager.rpc.address: hadoop01  #:后面一定要有空格
```

修改 workers 文件，将另外两台节点服务器添加为本 Flink 集群的 TaskManager 节点， 具体修改如下

```shell
vi workers 
hadoop02
hadoop03

vi master
hadoop01:8081
```

在 flink-conf.yaml 文件中还可以对集群中的 JobManager 和 TaskManager 组件 进行优化配置，主要配置项如下：

 ⚫ jobmanager.memory.process.size：对 JobManager 进程可使用到的全部内存进行配置， 包括 JVM 元空间和其他开销，默认为 1600M，可以根据集群规模进行适当调整。 

⚫ taskmanager.numberOfTaskSlots：对每个 TaskManager 能够分配的 slots 数量进行配置， 默认为 1，可根据 TaskManager 所在的机器能够提供给 Flink 的 CPU 数量决定。所谓 slots 就是 TaskManager 中具体运行一个任务所分配的计算资源。

 ⚫ parallelism.default：Flink 任务执行的默认并行度配置，优先级低于代码中进行的并行 度配置和任务提交时使用参数进行的并行度数量配置。

将mysql驱动包拷贝到lib目录下

将配置分发给其他两台节点：

```
scp -r /export/servers/flink/   hadoop03://export/servers/
```

```
bin/start-cluster.sh 
```

###### FLUME安装

```shell
<?xml version="1.0" encoding="UTF-8"?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.

-->
<Configuration status="ERROR">
  <Properties>
    <Property name="LOG_DIR">/export/servers/flume/log</Property>
  </Properties>
  <Appenders>
    <Console name="Console" target="SYSTEM_ERR">
      <PatternLayout pattern="%d (%t) [%p - %l] %m%n" />
    </Console>
    <RollingFile name="LogFile" fileName="${LOG_DIR}/flume.log" filePattern="${LOG_DIR}/archive/flume.log.%d{yyyyMMdd}-%i">
      <PatternLayout pattern="%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %equals{%x}{[]}{} - %m%n" />
      <Policies>
        <!-- Roll every night at midnight or when the file reaches 100MB -->
        <SizeBasedTriggeringPolicy size="100 MB"/>
        <CronTriggeringPolicy schedule="0 0 0 * * ?"/>
      </Policies>
      <DefaultRolloverStrategy min="1" max="20">
        <Delete basePath="${LOG_DIR}/archive">
          <!-- Nested conditions: the inner condition is only evaluated on files for which the outer conditions are true. -->
          <IfFileName glob="flume.log.*">
            <!-- Only allow 1 GB of files to accumulate -->
            <IfAccumulatedFileSize exceeds="1 GB"/>
          </IfFileName>
        </Delete>
      </DefaultRolloverStrategy>
    </RollingFile>
  </Appenders>

  <Loggers>
    <Logger name="org.apache.flume.lifecycle" level="info"/>
    <Logger name="org.jboss" level="WARN"/>
    <Logger name="org.apache.avro.ipc.netty.NettyTransceiver" level="WARN"/>
    <Logger name="org.apache.hadoop" level="INFO"/>
<Logger name="org.apache.hadoop.hive" level="ERROR"/>
# 引入控制台输出，方便学习查看日志
    <Root level="INFO">
      <AppenderRef ref="LogFile" />
      <AppenderRef ref="Console" />
    </Root>
  </Loggers>

</Configuration>

```

###### KAFKA安装

```
#broker的全局唯一编号，不能重复，只能是数字。
broker.id=0

#broker对外暴露的IP和端口 （每个节点单独配置）
advertised.listeners=PLAINTEXT://hadoop01:9092
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的线程数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用"，"分隔
log.dirs=/export/servers/kafka/datas
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
# 每个topic创建时的副本数，默认时1个副本
offsets.topic.replication.factor=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#每个segment文件的大小，默认最大1G
log.segment.bytes=1073741824
# 检查过期数据的时间，默认5分钟检查一次是否数据过期
log.retention.check.interval.ms=300000
#配置连接Zookeeper集群地址（在zk根目录下创建/kafka，方便管理）
zookeeper.connect=hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka

```

分发

```
server.properties中的broker.id及advertised.listeners
	注：broker.id不得重复，整个集群中唯一。
```

```
#! /bin/bash

case $1 in
"start"){
    for i in  hadoop01 hadoop02 hadoop03
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/export/servers/kafka/bin/kafka-server-start.sh -daemon /export/servers/kafka/config/server.properties"
    done
};;
"stop"){
    for i in  hadoop01 hadoop02 hadoop03
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/export/servers/kafka/bin/kafka-server-stop.sh "
    done
};;
esac
```

chmod 777

###### 帆软

```shell
wget -c https://fine-build.oss-cn-shanghai.aliyuncs.com/finebi/6.0/stable/exe/spider/linux_unix_FineBI6_0-CN.sh
```

```
chmod 777 linux_unix_FineBI6_0-CN.sh
```

```
bash linux_unix_FineBI6_0-CN.sh
```

```
http://hadoop01:37799/webroot/decision\
/export/servers/FineBI6.0/bin
```

```
等在FineBI启动后，便可访问其页面了，访问地址如下：
http://hadoop01:37799/webroot/decision\
设置账号密码
root
123456
```

```
创建帆软源数据库
CREATE DATABASE `finedb` DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
```

选择外联数据库

![image-20230512143739742](D:\集群规划\大数据集群搭建.assets\image-20230512143739742.png)

添加驱动测试连接

![image-20230512152603597](D:\集群规划\大数据集群搭建.assets\image-20230512152603597.png)

从数据库导入设置直联

![image-20230512152846388](D:\集群规划\大数据集群搭建.assets\image-20230512152846388.png)

###### HBASE安装

hbase-env.sh

```
export JAVA_HOME=/export/servers/jdk/
# 找到文件底部的 HBASE_MANAGES_ZK 参数，取消注释并设置为 false (默认为true)
# (该配置项说明：是否使用HBase自带的zookeeper，需要设置为否)
export HBASE_MANAGES_ZK=false
```

hbase-stie.xml

```
<configuration>
<!-- 指定HDFS的路径 -->
<property>
<name>hbase.rootdir</name>
<value>hdfs://hadoop01:9000/hbase</value>
<!-- 注：hdfs://hadoop01:9000/hbase必须与Hadoop集群的core-site.xml文件配置中的端口号保
持一致，并且该项不识别IP，只能使用hostname。 -->
</property>
<!-- 指定启用分布式集群模式 -->
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>
<!-- 指定zookeeper集群 -->
<property>
<name>hbase.zookeeper.quorum</name>
<value>hadoop01,hadoop02,hadoop03</value>
</property>
<!-- 指定zookeeper的临时文件夹 -->
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/export/servers/hbase/dataDir</value>
</property>
</configuration>
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property>
```

regionservers

```
hadoop01
hadoop02
hadoop03
```

分发三台，启动hadoop zk hbase

启动成功hadoop01出现HMaster HRegionServer

其余两台HRegionServer

```
遇到hmaster启动消失删除下面任意一个jar 注意版本兼容性 然后重启
```

![image-20230513160344711](D:\集群规划\大数据集群搭建.assets\image-20230513160344711.png)

java.lang.IllegalStateException: The procedure WAL relies on the ability to hsync for proper operation during component failures, but the underlying filesystem does not support doing so. Please check the config value of 'hbase.procedure.store.wal.use.hsync' to set the desired level of robustness and ensure the config value of 'hbase.wal.dir' points to a FileSystem mount that can provide it.

hbase-site新增

```
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property>
```

# 大数据

## HADOOP

### HDFS读写流程

```
hdfs --读：
1.客户端filesys访问namenode需要读的目录文件，看是否存在
2.namenode给出反馈
3.访问datanode1请求读取数据，datanode将数据信息返回给filesys
4.分别读dn2、dn3

hdfs --写：
1.客户端filesys访问namenode需要读的目录文件，看是否存在
2.namenode给出反馈
3.请求上传一个0-128m的block
4.返回dn1、dn2...将这些节点作为存储节点
5.filesys依次对dn节点建立传输通道，dn节点做出应答
6.然后进行数据传输
```

```
HDFS写入流程时候，某台dataNode挂掉如何运行？
	当DataNode突然挂掉了，客户端接收不到这个DataNode发送的ack(确认机制)确认，客户端会通知NameNode，NameNode检查并确认该块的副本与规定的不符，NameNode会通知闲置的DataNode去复制副本，并将挂掉的DataNode作下线处理。等挂掉的DataNode节点恢复后, 删除该节点中曾经拷贝的不完整副本数据。
```

### HDFS小文件处理

```
1.影响
   浪费资源
   1个maptask 1G
   1个文件块，占用namenode多大内存150字节
   128G能存储多少文件块？   128 g* 1024m*1024kb*1024byte/150字节 = 9.1亿文件块
2.解决
  （1）har
   (2)CombineTextInputFormat
   (3)重写分区
   (4)JVM重用  默认10
```

### MAPREDUCE

```scala
sc.textFile("/input")      			
  .flatMap(_.split(" "))
  .map((_, 1))                          
  .reduceByKey(_ + _)
  .saveAsTextFile("/output")
```

map任务 

1.读取数据将每一行解析成<key,value>

​			注：key是当前行的起始位置，单位是字节。第一行的起始位置是0，value是当前行的内容。有多少行
就产生多少键值对。每个键值对调用一个map函数  (map函数为map任务中的一个函数)

2.采用map转换成自己想要的<key,value>

3.对新的<key,value>进行排序  默认分区1

4.根据不同的数据进行分区排序  （快排）

5.(可选)分组后的数据进行归约。在map段开始Combine是为了防止数据倾斜

reduce任务

 1 对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点。

 注：那哪些map任务进入到那些reduce节点，原则是按照分区。 

2 对多个map任务的输出进行合并、排序。写reduce函数自己的业务逻辑，对输入的key2、values2处 理，转换成新的key3、value3输出。 

注：为什么需要合并操作？因为需要将多个map任务输出的结果进行合并，合并之后既可以排序。 分组前后，键值对的数目有变化吗？答案：没有变化。 

3 把reduce的输出保存到文件(HDFS)中。

##### ![image-20230701003303456](D:\集群规划\大数据集群搭建.assets\image-20230701003303456.png)

![image-20230701003422877](D:\集群规划\大数据集群搭建.assets\image-20230701003422877.png)

### YARN

![](D:\集群规划\大数据集群搭建.assets\1.png)

## FLUME

![image-20230701004704047](D:\集群规划\大数据集群搭建.assets\image-20230701004704047.png)

## HIVE

```
HQL mr执行流程

1.将hql解析为抽象语法树
2.通过解析器执行逻辑执行计划
3.通过优化器调优
4.通过解析器执行物质执行计划 spark/mr
5.然后提交到yarn执行
```



## 数仓建设

![image-20230705120236056](D:\集群规划\大数据集群搭建.assets\image-20230705120236056.png)

### 架构选型--Lambda架构

![image-20230703231635395](D:\集群规划\大数据集群搭建.assets\image-20230703231635395.png)

![image-20230701010440802](D:\集群规划\大数据集群搭建.assets\image-20230701010440802.png)

总线矩阵：指以一致性维度为列，以业务过程为行，构建业务的数据矩阵，通过标记表示该维度与业务过程的相关性

拉链表：记录历史数据，记录一个事物从开始一直到当前状态的所有变化的信息

### 业务分析以及主体域划分

人力资源业务线
	sop（业务流程）包括员工从招聘到入职在职离职整体生命周期，同时也包含集团内部运营、行政管理

**人才招聘**
	招聘岗位发布
	招聘渠道
	简历
	应聘
	面试
	招聘体验
	体检
	offer
	背调

**员工管理**   员工接offer->预计入职->体检->被调->入职->组织架构确定->合同签署->账号开通->入职体验调查
	入职
	试用期
	绩效
	晋升
	考勤    员工考勤日上班打卡->中间打卡->下班打卡->考勤结果分析->加班
	调动
	学习发展
	离职
**组织发展**
	组织架构
	干部考量
	敬业度/满意度
**运营管理**
	行政
	ssc（共享服务中心是通过对人员、技术和流程的有效整合，实现组织内公共流程的标准化和精简化的一种创新手段）
	内部社区运营
**数据管理**
	元数据
	数据质量
	工单
	指标系统
	数据治理

![image-20230703232603079](D:\集群规划\大数据集群搭建.assets\image-20230703232603079.png)

![image-20230703232611724](D:\集群规划\大数据集群搭建.assets\image-20230703232611724.png)

### 数仓标准制定

#### 分层设计

ODS（接入层）
	全称Operational Data Store，ODS层是最接近数据源的一层，从数据源（api、数据库等）将数据同步数仓中，中间不做任何处理操作
DIM（维度）
	全称Dimension，用于存放维度数据，在dwd层做维度退化
DWD（明细层）
	全称Data Warehouse Detail，是数仓明细数据层，对ODS层的数据进行关联，清洗，维度退化（将维度表中维度数据放入明细表中），转换，主题域建设等操作
DWS（汇总层）
	全称Data WareHouse Service，按照主题域、颗粒度（例如员工、部门）划分，按照周期粒度、维度聚合形成指标较多的宽表，用于提供后续的业务查询，数据应用，最重要一点需要在DWS层完成指标口径统一及沉淀
ADS（应用层）
	全称Applacation data service，按照应用域，颗粒度划分（例如员工、部门）划分，按照应用主题将对应数据标签补充至应用层，最终形成用户画像及专项应用

#### dqc，元数据规范，参数规范,其他规范等（调参）

#### 模型建设5要素 

​	**数据域**         **员工管理**   员工接offer->预计入职->体检->被调->入职->组织架构确定->合同签署->账号开通->入职体验调查
​		对当前业务场景或业务sop进行拆分完成对应数据建设
​	**事实表设计**
​		围绕着业务过程来设计，通过获取描述业务过程的度量来表达业务过程，包含了引用的维度和与业务过程有关的度量。
​	**维度**   
​		对当前场景描述及补充     
​	**颗粒度**
​		主题域下场景用户再进行细致拆分（例如用户可拆分为买家或卖家），颗粒度必须拆分为不可拆分的状态（例如用户拆分为买家，买家不可拆分）
​	**度量值**
​		对场景下数值类型的数据记录

### 数据基建--基础建设

**数仓初始期**
1.明确组织架构业务流程及内容 

2.根据业务场景制定架构方向及技术栈选型或者买平台具体资源要根据部门预算调整 

3.对当前业务梳理 明确数据源及数据域、主题域划分

4.确定一期要做的大方向 数据版图梳理okr制定 

5.业务及主题域命名规范、数仓分层规范、元数据规范、模型建设规范、流程制定制定 

6.确认核心指标口径，完成核心数据表接入及模型建设ods->dwd->dws 

7.根据业务场景完成应用层场景数据模型建设支撑下游1期的专题报告及看板搭建

模型建设5要素
	颗粒度
		员工
	事实表类型
		事务性事实表（周期-最近1年）
	数据域
		晋升
	派生指标
		评委人数、评委相关的平均评分
	维度
		员工本身=颗粒度



项目背景：

集团每年都会为员工提供晋升通道，保障优质员工在专业及管理能力侧能够提升自己，从而使公司业务及员工能力有所发展，但目前缺乏晋升类数据支撑，导致晋升流程合规性以及评审情况无法得到保障，从而无法看清晋升具体情况

背景就是你做的这个事 为啥要做 要解决什么样的问题，比如晋升这个如图，流程是你是怎么做 比如说1先调研需求 跟bi他们一起分析需求背景 以及要做的模块指标 2数据接入ods到dwd 3cdm（dwd+dws）层建设 这里突出模型5要素 你是怎么规划建模的 4应用层开发 5报表侧或者其他系统数据支持



晋升活动开始->提名条件->员工提名->提交材料->评审（材料评审可免答辩）->评审结果（综合结果与点评）->申诉



数据仓库的基石，控制好数据质量，是做数据仓库基本要求，也使得下游业务方对数据用的放心
数仓建设过程中常遇到几类问题:
1. 开发未规范化&对业务了解不足

2. 数据链路缺少卡点保障

2. 数据运维不及时

4. 业务数据问题处理无机制

  针对如上几类问题开展全链路数据质量保障措
  施，构建“质量可靠、安全稳定“的数据质量体系

  

  5.6全链路数据质量保障项目
  项目流程:

  方案措施1-上线/变更规范: 与同事leader制定模型上线、模型变更、指标变更等规范解决开发未规范化问题，降低开发风险

  方案措施2- 数据质量监控DQC建设: 增强业务数据质量保障，提升数据监控覆盖度和准确度，减少线上问题发生，完成数据链路模型基础监控100%覆盖，解决数据链路缺少卡点保障痛点，保障高质量数据向下输出

  方案措施3- 数据基线及SLA搭建: 建设各场景任务基线，设置值班报警，并制定出保障-容灾备份方案，保障核心任务产出的及时性，稳定性，解决数据运维不及时痛点D

  方案措施4- 容灾备份快恢能力: 具备容灾备份快恢能力 (这里指全量) ，临时给下游临时任务切换为T-2数据，恢复整体任务进行，但数据资产、数据应用模型较多不能顾全还容易出现误操作情况，所以需要容灾备份任务还原所有数据资产，保障SLA补破线能够及时交付

  方案措施5- 数据问题上报: 通过数据问题上报平台建设对当前数据问题记录以及让业务方同时关注问题进度，达到双方知会效果
  项目产出:
  1.完成3类上线/变更规范措施建设，模型月返工率由50%降低至20%
  2.完成数据链路模型基础监控 100%覆盖，月均拦截10+次，防止问题数据向下输送3.建设6条应用场景任务基线，建设周值班表，保障核心任务产出的及时性，稳定性，月均基线告警降低至6次，破线1次4.建设问题上报平台，之前近500+存量数据问题统一管理存放，提升下游易找易复用，实现问题全流程跟踪
  思考: 全链路数据保障是整个数仓中的核心，好的数据质量基建要从每一个流程[需求分析->开发->提交/发布->应用] 都有相应的数据质量保障卡点，保障流程中每一步都准确衔接。如果大家都能遵守流程中每一步去执行，能降低线上问题产生频率，提升下游整体用数信心。



## 数据治理

**模型合规**（包括了元数据治理，原来只到了元数据层次）：

1.数据标准重制定及修复，包括对原来数据域重构，表字段命名体系重构，并对原来模型按照新标准合规改造 

 2.元数据补充 owner、使用说明、字段中文名具体内容、颗粒度声明、主键声明等补充保障下游及内部使用时候清晰

 3.制度建设：完善模型评审制度、代码提交强审核，保障内容合规后上线 

4.分层合理性，治理不规范的模型分层引用，例如ADS层表依赖了非DWS层的表，建议优化  

5.数据链路合理性（本次新补充内容）：减少因内容不足产出烟囱模型，从而相互依赖加长链路情况

**数据质量合规**：

1.流程化，任务上线/变更流程，指标变更流程 

2.dqc管控：对原4大基础dqc进行补充以及核心业务模型dqc补充，并对原无效dqc下线，对常触发dqc进行调整（例如表行数波动，可通过算法对近7天数据量监测） 

3.sla及基线治理（这里也可以放到人员运维roi治理中）：上线前把控，保障基线正常运行，核心任务优先产出且分配高资源，培训及整理值班运维手册，建设容灾备份快恢能力临时修复数据 

4.上游问题数据治理：数据质量长期监测体系（详情见课程3-数据质量课件）

**数据安全合规：**

1.角色权限管控，对不同使用/开发角色提供不同使用权限，根据报表、看板的权限等级，在同一个图表中限制不同的用户能够看到的数据也不一样(常用于报表各模块内容展示) 

2.数据脱敏，通过脱敏防止数据泄漏 

3.表/字段分级：对每个表及字段进行打标，保障每张表都有数据安全管控 

4.数据权限使用 表/字段走审批流程 并设置数据使用申请时卡点负责人/组  

5.其他 数据下载管控（一般来说最多下载1000行/次），离职数据风险管控等等

**存储资源治理：**

1.设置统一表生命周期，并对当前表按照新标准裁剪，对未分区表重制定分区 

2.长期未引用/被使用/临时的表下线

 3.压缩格式/存储格式优化 

4.根据业务对表存储重划分：对较大数据量表可以采取全量转增量操作、拉链表操作

**计算资源治理**：

1.数据倾斜任务治理（后面我会细讲治理方法 这里跳过）

2.消耗大core/内存任务治理 

3.无效监控项、重复开发情况占用计算资源、数据价值低的模型占用计算资源及时下线 

4.梳理数据链路并对任务调度治理 

5.规划核心任务 并分配任务执行优先级 把非核心的任务靠后运行 

6.小文件治理

 7.其他 例如hive spark2 切换spark3采用aqe特性  采用z-order+spark排序算法解决join时读取效果提升

数据价值治理 （整体来说是提升模型复用性）：

1.烟囱数据模型及对应任务、模型粒度重复及时下线 

2.ads指标下沉到dws  

3.建立模型价值度指标，持续下线低价值模型 

4.下线ads层对应业务不再使用的场景模型

**人力成本治理**（这个可以不说）：

1.指导培训组员技术侧/业务侧能力能够独挡一面，并完善文档沉淀帮助后续新人培训开展 

2.让熟悉不同数据域的组员安排在合理的数据域范围，同时做backup 

3.建立相应需求开发流程机制，统计人员产出效率，方便针对性安排后续开发内容

4.为当前需求及项目难度打分，帮助大家更好意识到项目能否落地、以及产出时间，同时衡量每人产出roi  
