------

# 大数据集群搭建

#### 集群启动

nn

http://hadoop01:9870/

2nn

http://hadoop02:9868/

rm

http://hadoop01:8088/

job

http://hadoop01:19888/

flink

http://hadoop01:8081/

hbase

http://hadoop01:16010/

帆软
http://hadoop01:37799/webroot/decision\

```shell
start-all.sh
mapred --daemon start historyserver
idea连接hive
hive --service metastore &
使用dbserver链接hive
hive --service hiveserver2 &
zk.sh start 
kf.sh start

/export/servers/FineBI6.0/bin

start-hbase.sh
```



#### 网络规划

|   节点   |       ip        | 节点类型 |
| :------: | :-------------: | :------: |
| hadoop01 | 192.168.202.134 |  master  |
| hadoop02 | 192.168.202.135 |  slave1  |
| hadoop03 | 192.168.202.136 |  slave2  |

#### 节点规划

|      服务       | Hadoop01 | Hadoop02 | Hadoop03 |
| :-------------: | :------: | :------: | :------: |
|    namenode     |    √     |          |          |
| secondNamenode  |          |    √     |          |
| resourcemanager |    √     |          |          |
|    datanode     |    √     |    √     |    √     |
|   nodemanager   |    √     |    √     |    √     |
|   jobhistory    |    √     |          |          |

#### 组件规划

|          |                             组件                             |
| :------: | :----------------------------------------------------------: |
| Hadoop01 | mysql,hive,spark,zk,ck,maxwell,hbase,flume,kafka,flink,redis，帆软 |
| Hadoop02 |                     zk,kafka,flink,hbase                     |
| Hadoop03 |                     zk,kafka,flink,hbase                     |

![image-20230522115620191](D:\集群规划\大数据集群搭建.assets\image-20230522115620191.png)

#### 搭建流程

###### 防火墙

```shell
查看防火墙状态
systemctl status firewalld
关闭防火墙
systemctl stop firewalld
```

###### 分发

```shell
cp 
scp -r 
```

###### 网络搭建

```shell
CentOS7和8里面，网络配置文件为：/etc/sysconfig/network-scripts/ifcfg-ens33  然后重启网卡

TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"

BOOTPROTO="static"

DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="fe58a64d-bd63-4195-873a-c3125b91f7bf"
DEVICE="ens33"
ONBOOT="yes"

IPADDR=192.168.202.134
NETMASK=255.255.255.0
GATEWAY=192.168.202.2
DNS1=8.8.8.8
```

```shell
vi  /etc/hosts

192.168.202.134 hadoop01
192.168.202.135 hadoop02
192.168.202.136 hadoop03
```

###### 修改主机名

```shell
hostnamectl set-hostname <newhostname>
bash 
```

###### 免密登录

```shell
ssh-keygen -t rsa  三台
三次回车
3台节点向其他两台外加本身  总共免密9次
ssh-copy-id hadoop01
ssh hadoop02 切换
```



###### 组件配置环境变量

######  vi /etc/profiled

```shell
export B_HOME=/export/servers
# Java
export JAVA_HOME=$B_HOME/jdk
export PATH=$PATH:$JAVA_HOME/bin
# Zookeeper
export ZOOKEEPER_HOME=$B_HOME/zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
# Hadoop
export HADOOP_HOME=$B_HOME/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
# HIVE
export HIVE_HOME=$B_HOME/hive
export PATH=$PATH:$HIVE_HOME/bin
# Spark
export SPARK_HOME=$B_HOME/spark
export PATH=$PATH:$SPARK_HOME/bin
# Kafka
export KAFKA_HOME=$B_HOME/kafka
export PATH=$PATH:$KAFKA_HOME/bin
# Flume
export FLUME_HOME=$B_HOME/flume
export PATH=$PATH:$FLUME_HOME/bin
# Sqoop
export SQOOP_HOME=$B_HOME/sqoop
export PATH=$PATH:$SQOOP_HOME/bin
# HBase
export HBASE_HOME=$B_HOME/hbase
export PATH=$PATH:$HBASE_HOME/bin
# Scala
export SCALA_HOME=$B_HOME/scala
export PATH=$PATH:$SCALA_HOME/bin
# Flink
export FLINK_HOME=$B_HOME/flink
export PATH=$PATH:$FLINK_HOME/bin


export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

```shell
 **source /etc/profiled**
```

###### JDK配置

```shell
tar -zxvf 文件名 -C 目标文件
java -version
```

###### HADOOP完全分布式配置

```shell
hadoop version 

修改配置文件
hadoop-env.sh
export JAVA_HOME=/export/servers/jdk

hdfs-site.xml
<configuration>
	<!--web UI-->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>

    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop02:9868</value>
    </property>
    <!--副本数-->
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
</configuration>

core-site.xml
<configuration>

<property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9000</value>
</property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/export/servers/hadoop/tmp</value>
</property>
<property>
		<name>hadoop.proxyuser.root.hosts</name>  
		<value>*</value> 
	</property>
	<property>
		<name>hadoop.proxyuser.root.groups</name>
		<value>*</value>
	</property>
    
</configuration>


mapred-site.xml
<configuration>

    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <!--历史服务器-->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop01:19888</value>
</property>
</configuration>

workers  datanode
hadoop01
hadoop02
hadoop03

yarn-site.xml
<configuration>

<!-- Site specific YARN configuration properties -->
   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop01</value>
    </property>

    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <!-- 一个容器获得的最小内存 -->
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <!-- 一个容器获得的最大内存 -->
        <value>6144</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <!-- 该节点上YARN可使用的物理内存总量 -->
        <value>6144</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个 -->
        <value>8</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <!-- 容器最小CPU核数，默认1个 -->
        <value>1</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <!-- 容器最大CPU核数，默认4个 -->
        <value>8</value>
    </property>
	<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
	<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

</configuration>
```

```shell
格式化 hdfs namenode - format 或 hadoop namenode - format
start-all.sh
mapred --daemon start historyserver
```

###### MYSQL安装

```shell
检查是否安装mysql
rpm -qa | grep mysql
rpm -e name 删除
下载mysql57-community-release-el7-7.noarch.rpm

安装MySQL包
yum -y install mysql57-community-release-el7-7.noarch.rpm
安装mysql需要禁掉GPG验证检查
yum update
yum -y install mysql-community-server --nogpgcheck
修改权限
chown -R mysql:mysql /var/lib/mysql
mysql初始化
mysqld --initialize
mysql启动
systemctl start mysqld
验证mysql安装
mysqladmin --version
登陆验证
第一次一般不需要密码 mysql直接进去 如果出现需要密码登陆报错请参考
https://blog.csdn.net/qq_54737884/article/details/122410476?spm=1001.2014.3001.5502
```

```shell
mysql报错
mysql5.0设置密码报错：ERROR 1193 (HY000): Unknown system variable
vi /etc/my.cnf
在mysqld加入
skip-grant-tables  #root免密登录
plugin-load-add=validate_password.so  #插件的加载方法，每次服务器启动时都必须给出该选项；
validate-password=FORCE_PLUS_PERMANENT #validate-password在服务器启动时使用该选项来控制插件的激活。
重启mysql
systemctl restart mysqld

设置密码等级
set global validate_password_policy=0;
查看安全策略 
SHOW VARIABLES LIKE 'validate_password%';
设置密码长度
set global validate_password_length=4;
设置新密码
SET PASSWORD = PASSwORD ('123456');
查看密码长度
select @@validate_password_length;
设置远程访问授权
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
刷新权限
flush privileges
```

###### HIVE安装

```shell
hive-site.xml

<configuration>
<!-- jdbc 连接的 URL -->
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
</property>
<!-- jdbc 连接的 Driver-->
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<!-- jdbc 连接的 username-->
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
</property>
<!-- jdbc 连接的 password -->
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
</property>
<!-- Hive 默认在 HDFS 的工作目录 -->
<property>
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
</property>
<!-- 指定存储元数据要连接的地址 -->
<property>
<name>hive.metastore.uris</name>
<value>thrift://hadoop01:9083</value>
</property>
<!--显示数据库名列明-->
<property>
<name>hive.cli.print.current.db</name>
<value>true</value>
</property>
<property>
<name>hive.cli.print.header</name>
<value>true</value>
</property>
</configuration>

将mysql驱动包拷贝到hive/lib目录下

初始化hive元数据库
schematool -dbType mysql -initSchema
初始化错误Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)
解决请参考厦门大学林子雨实验室
https://dblab.xmu.edu.cn/blog/?s=hive
初始化成功会出现
Initialization script completed
schemaTool completed

出现
FAILED: HiveException java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
启动hive元数据服务
hive --service metastore
到此hive配置完成
```

###### SPARK ON YARN安装

```
将hive/conf/hive-site.xml 拷贝到 spark/conf/下
将mysql驱动包拷贝jars下

修改spark-env.sh
export JAVA_HOME=/export/servers/jdk
export SPARK_CONF_DIR=/export/servers/spark/conf
export HADOOP_CONF_DIR=/export/servers/hadoop
export YARN_CONF_DIR=/export/servers/hadoop/etc/hadoop

bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master yarn  \
/export/servers/spark/examples/jars/spark-examples_2.12-3.1.1.jar 100
```

如果是本地idea测试 需要在本地设置hadoop的环境变量 其中要跟换二进制文件 winutils 

还需要配置本地hosts

需要将hive-site,core-site,hdfs-site,yarn-site拷贝到resouce下并配置log4j

```
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd 
HH:mm:ss} %p %c{1}: %m%n
# Set the default spark-shell log level to ERROR. When running the spark-shell,
the
# log level for this class is used to overwrite the root logger's log level, so
that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=ERROR
# Settings to quiet third party logs that are too verbose
log4j.logger.org.spark_project.jetty=ERROR
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR
# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent
UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR
```

```scala
import org.apache.spark.sql.SparkSession

/**
 * @program: BigData
 * @description: ${description}
 * @author: 逍遥哥哥每天都要努力啊
 * @create: 2023/4/28
 * */
object test {
  def main(args: Array[String]): Unit = {
    System.setProperty("HADOOP_USER_NAME", "root")

    val spark = SparkSession
      .builder()
      .master("local[*]")
      .appName("regiontopthree")
      .config("spark.sql.warehouse.dir", "hdfs://hadoop01:9000/user/hive/warehouse")
      .config("hive.metastore.uris", "thrift://hadoop01:9083")
      .enableHiveSupport()
      .getOrCreate()


    spark.sql("show databases").show(false)

    spark.stop()
  }

}

```



###### ZOOKEEPER安装

```
vi z00.cfg
# 心跳检查的时间 2秒
tickTime=2000
# 初始化时 连接到服务器端的间隔次数，总时间10*2=20秒
initLimit=10
# ZK Leader 和follower 之间通讯的次数，总时间5*2=10秒
syncLimit=5
# 存储内存中数据快照的位置，如果不设置参数，更新事务日志将被存储到默认位置。
dataDir=/export/servers/zookeeper/ZKdata
# ZK 服务器端的监听端口  
clientPort=2181
# (心跳端口:选举端口)
server.1=hadoop01:2888:3888
server.2=hadoop02:2888:3888
server.3=hadoop03:2888:3888

vi myid  
1
分发三台
2 3
```

```shell
#!/bin/bash

case $1 in
"start"){
	for i in hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 启动 ------------
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh start"
	done
};;
"stop"){
	for i in  hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 停止 ------------    
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh stop"
	done
};;
"status"){
	for i in  hadoop01 hadoop02 hadoop03
	do
        echo ---------- zookeeper $i 状态 ------------    
		ssh $i "/export/servers/zookeeper/bin/zkServer.sh status"
	done
};;
esac

```

```
 chmod 777 zk.sh
```



###### FLINK本地模式

```shell
tar -zxvf
bin/start-cluster.sh
bin/stop-cluster.sh

[root@hadoop01 flink]# jps
3073 Jps
3014 TaskManagerRunner
2701 StandaloneSessionClusterEntrypoint
```

###### FLINK集群模式

进入 conf 目录下，修改 flink-conf.yaml 文件，修改 jobmanager.rpc.address 参数为 hadoop01  

这就指定了 hadoop01 节点服务器为 JobManager 节点。

```
jobmanager.rpc.address: hadoop01  #:后面一定要有空格
```

修改 workers 文件，将另外两台节点服务器添加为本 Flink 集群的 TaskManager 节点， 具体修改如下

```shell
vi workers 
hadoop02
hadoop03

vi master
hadoop01:8081
```

在 flink-conf.yaml 文件中还可以对集群中的 JobManager 和 TaskManager 组件 进行优化配置，主要配置项如下：

 ⚫ jobmanager.memory.process.size：对 JobManager 进程可使用到的全部内存进行配置， 包括 JVM 元空间和其他开销，默认为 1600M，可以根据集群规模进行适当调整。 

⚫ taskmanager.numberOfTaskSlots：对每个 TaskManager 能够分配的 slots 数量进行配置， 默认为 1，可根据 TaskManager 所在的机器能够提供给 Flink 的 CPU 数量决定。所谓 slots 就是 TaskManager 中具体运行一个任务所分配的计算资源。

 ⚫ parallelism.default：Flink 任务执行的默认并行度配置，优先级低于代码中进行的并行 度配置和任务提交时使用参数进行的并行度数量配置。

将mysql驱动包拷贝到lib目录下

将配置分发给其他两台节点：

```
scp -r /export/servers/flink/   hadoop03://export/servers/
```

```
bin/start-cluster.sh 
```

###### FLUME安装

```shell
<?xml version="1.0" encoding="UTF-8"?>
<!--
 Licensed to the Apache Software Foundation (ASF) under one or more
 contributor license agreements.  See the NOTICE file distributed with
 this work for additional information regarding copyright ownership.
 The ASF licenses this file to You under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.

-->
<Configuration status="ERROR">
  <Properties>
    <Property name="LOG_DIR">/export/servers/flume/log</Property>
  </Properties>
  <Appenders>
    <Console name="Console" target="SYSTEM_ERR">
      <PatternLayout pattern="%d (%t) [%p - %l] %m%n" />
    </Console>
    <RollingFile name="LogFile" fileName="${LOG_DIR}/flume.log" filePattern="${LOG_DIR}/archive/flume.log.%d{yyyyMMdd}-%i">
      <PatternLayout pattern="%d{dd MMM yyyy HH:mm:ss,SSS} %-5p [%t] (%C.%M:%L) %equals{%x}{[]}{} - %m%n" />
      <Policies>
        <!-- Roll every night at midnight or when the file reaches 100MB -->
        <SizeBasedTriggeringPolicy size="100 MB"/>
        <CronTriggeringPolicy schedule="0 0 0 * * ?"/>
      </Policies>
      <DefaultRolloverStrategy min="1" max="20">
        <Delete basePath="${LOG_DIR}/archive">
          <!-- Nested conditions: the inner condition is only evaluated on files for which the outer conditions are true. -->
          <IfFileName glob="flume.log.*">
            <!-- Only allow 1 GB of files to accumulate -->
            <IfAccumulatedFileSize exceeds="1 GB"/>
          </IfFileName>
        </Delete>
      </DefaultRolloverStrategy>
    </RollingFile>
  </Appenders>

  <Loggers>
    <Logger name="org.apache.flume.lifecycle" level="info"/>
    <Logger name="org.jboss" level="WARN"/>
    <Logger name="org.apache.avro.ipc.netty.NettyTransceiver" level="WARN"/>
    <Logger name="org.apache.hadoop" level="INFO"/>
<Logger name="org.apache.hadoop.hive" level="ERROR"/>
# 引入控制台输出，方便学习查看日志
    <Root level="INFO">
      <AppenderRef ref="LogFile" />
      <AppenderRef ref="Console" />
    </Root>
  </Loggers>

</Configuration>

```

###### KAFKA安装

```
#broker的全局唯一编号，不能重复，只能是数字。
broker.id=0

#broker对外暴露的IP和端口 （每个节点单独配置）
advertised.listeners=PLAINTEXT://hadoop01:9092
#处理网络请求的线程数量
num.network.threads=3
#用来处理磁盘IO的线程数量
num.io.threads=8
#发送套接字的缓冲区大小
socket.send.buffer.bytes=102400
#接收套接字的缓冲区大小
socket.receive.buffer.bytes=102400
#请求套接字的缓冲区大小
socket.request.max.bytes=104857600
#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用"，"分隔
log.dirs=/export/servers/kafka/datas
#topic在当前broker上的分区个数
num.partitions=1
#用来恢复和清理data下数据的线程数量
num.recovery.threads.per.data.dir=1
# 每个topic创建时的副本数，默认时1个副本
offsets.topic.replication.factor=1
#segment文件保留的最长时间，超时将被删除
log.retention.hours=168
#每个segment文件的大小，默认最大1G
log.segment.bytes=1073741824
# 检查过期数据的时间，默认5分钟检查一次是否数据过期
log.retention.check.interval.ms=300000
#配置连接Zookeeper集群地址（在zk根目录下创建/kafka，方便管理）
zookeeper.connect=hadoop01:2181,hadoop02:2181,hadoop03:2181/kafka

```

分发

```
server.properties中的broker.id及advertised.listeners
	注：broker.id不得重复，整个集群中唯一。
```

```
#! /bin/bash

case $1 in
"start"){
    for i in  hadoop01 hadoop02 hadoop03
    do
        echo " --------启动 $i Kafka-------"
        ssh $i "/export/servers/kafka/bin/kafka-server-start.sh -daemon /export/servers/kafka/config/server.properties"
    done
};;
"stop"){
    for i in  hadoop01 hadoop02 hadoop03
    do
        echo " --------停止 $i Kafka-------"
        ssh $i "/export/servers/kafka/bin/kafka-server-stop.sh "
    done
};;
esac
```

chmod 777

###### 帆软

```shell
wget -c https://fine-build.oss-cn-shanghai.aliyuncs.com/finebi/6.0/stable/exe/spider/linux_unix_FineBI6_0-CN.sh
```

```
chmod 777 linux_unix_FineBI6_0-CN.sh
```

```
bash linux_unix_FineBI6_0-CN.sh
```

```
http://hadoop01:37799/webroot/decision\
/export/servers/FineBI6.0/bin
```

```
等在FineBI启动后，便可访问其页面了，访问地址如下：
http://hadoop01:37799/webroot/decision\
设置账号密码
root
123456
```

```
创建帆软源数据库
CREATE DATABASE `finedb` DEFAULT CHARACTER SET utf8 COLLATE utf8_bin;
```

选择外联数据库

![image-20230512143739742](D:\集群规划\大数据集群搭建.assets\image-20230512143739742.png)

添加驱动测试连接

![image-20230512152603597](D:\集群规划\大数据集群搭建.assets\image-20230512152603597.png)

从数据库导入设置直联

![image-20230512152846388](D:\集群规划\大数据集群搭建.assets\image-20230512152846388.png)

###### HBASE安装

hbase-env.sh

```
export JAVA_HOME=/export/servers/jdk/
# 找到文件底部的 HBASE_MANAGES_ZK 参数，取消注释并设置为 false (默认为true)
# (该配置项说明：是否使用HBase自带的zookeeper，需要设置为否)
export HBASE_MANAGES_ZK=false
```

hbase-stie.xml

```
<configuration>
<!-- 指定HDFS的路径 -->
<property>
<name>hbase.rootdir</name>
<value>hdfs://hadoop01:9000/hbase</value>
<!-- 注：hdfs://hadoop01:9000/hbase必须与Hadoop集群的core-site.xml文件配置中的端口号保
持一致，并且该项不识别IP，只能使用hostname。 -->
</property>
<!-- 指定启用分布式集群模式 -->
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>
<!-- 指定zookeeper集群 -->
<property>
<name>hbase.zookeeper.quorum</name>
<value>hadoop01,hadoop02,hadoop03</value>
</property>
<!-- 指定zookeeper的临时文件夹 -->
<property>
<name>hbase.zookeeper.property.dataDir</name>
<value>/export/servers/hbase/dataDir</value>
</property>
</configuration>
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property>
```

regionservers

```
hadoop01
hadoop02
hadoop03
```

分发三台，启动hadoop zk hbase

启动成功hadoop01出现HMaster HRegionServer

其余两台HRegionServer

```
遇到hmaster启动消失删除下面任意一个jar 注意版本兼容性 然后重启
```

![image-20230513160344711](D:\集群规划\大数据集群搭建.assets\image-20230513160344711.png)

java.lang.IllegalStateException: The procedure WAL relies on the ability to hsync for proper operation during component failures, but the underlying filesystem does not support doing so. Please check the config value of 'hbase.procedure.store.wal.use.hsync' to set the desired level of robustness and ensure the config value of 'hbase.wal.dir' points to a FileSystem mount that can provide it.

hbase-site新增

```
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property>
```

# 大数据

## HADOOP

### HDFS读写流程

```
hdfs --读：
1.客户端filesys访问namenode需要读的目录文件，看是否存在
2.namenode给出反馈
3.访问datanode1请求读取数据，datanode将数据信息返回给filesys
4.分别读dn2、dn3

hdfs --写：
1.客户端filesys访问namenode需要读的目录文件，看是否存在
2.namenode给出反馈
3.请求上传一个0-128m的block
4.返回dn1、dn2...将这些节点作为存储节点
5.filesys依次对dn节点建立传输通道，dn节点做出应答
6.然后进行数据传输
```

```
HDFS写入流程时候，某台dataNode挂掉如何运行？
	当DataNode突然挂掉了，客户端接收不到这个DataNode发送的ack(确认机制)确认，客户端会通知NameNode，NameNode检查并确认该块的副本与规定的不符，NameNode会通知闲置的DataNode去复制副本，并将挂掉的DataNode作下线处理。等挂掉的DataNode节点恢复后, 删除该节点中曾经拷贝的不完整副本数据。
```

### HDFS小文件处理

```
1.影响
   浪费资源
   1个maptask 1G
   1个文件块，占用namenode多大内存150字节
   128G能存储多少文件块？   128 g* 1024m*1024kb*1024byte/150字节 = 9.1亿文件块
2.解决
  （1）har
   (2)CombineTextInputFormat
   (3)重写分区
   (4)JVM重用  默认10
```

### MAPREDUCE

```scala
sc.textFile("/input")      			
  .flatMap(_.split(" "))
  .map((_, 1))                          
  .reduceByKey(_ + _)
  .saveAsTextFile("/output")
```

map任务 

1.读取数据将每一行解析成<key,value>

​			注：key是当前行的起始位置，单位是字节。第一行的起始位置是0，value是当前行的内容。有多少行
就产生多少键值对。每个键值对调用一个map函数  (map函数为map任务中的一个函数)

2.采用map转换成自己想要的<key,value>

3.对新的<key,value>进行排序  默认分区1

4.根据不同的数据进行分区排序  （快排）

5.(可选)分组后的数据进行归约。在map段开始Combine是为了防止数据倾斜

reduce任务

 1 对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点。

 注：那哪些map任务进入到那些reduce节点，原则是按照分区。 

2 对多个map任务的输出进行合并、排序。写reduce函数自己的业务逻辑，对输入的key2、values2处 理，转换成新的key3、value3输出。 

注：为什么需要合并操作？因为需要将多个map任务输出的结果进行合并，合并之后既可以排序。 分组前后，键值对的数目有变化吗？答案：没有变化。 

3 把reduce的输出保存到文件(HDFS)中。

##### ![image-20230701003303456](D:\集群规划\大数据集群搭建.assets\image-20230701003303456.png)

![image-20230701003422877](D:\集群规划\大数据集群搭建.assets\image-20230701003422877.png)

### YARN

![](D:\集群规划\大数据集群搭建.assets\1.png)

## FLUME

![image-20230701004704047](D:\集群规划\大数据集群搭建.assets\image-20230701004704047.png)

## HIVE

```
HQL mr执行流程

1.将hql解析为抽象语法树
2.通过解析器执行逻辑执行计划
3.通过优化器调优
4.通过解析器执行物质执行计划 spark/mr
5.然后提交到yarn执行
```
